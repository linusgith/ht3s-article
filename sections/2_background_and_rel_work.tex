\section{Background and Related Work}
\label{sec:background}

Although, there has been much research on FaaS and the paradigm has arrived in mainstream technology, with many providers creating their own offering. 
Be it running in large datacenters (e.g. AWS Lambda, Google Cloud Run Functions, Microsoft Azure Functions) \cite{8241104} or distributed at the edge (Cloudflare Workers, Fastly Compute) \cite{10643918}. 
At the same time, there are only few commercial offerings for FaaS on fog infrastructure \cite{8814084} and research on the topic is also more scarce.

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{graphs/paper-sketch.png}
    \caption{\textbf{Overview of an exemplary Fog system structure}, that shows nodes with varying sizes, where bigger nodes signal higher computational power. They are classified as \textit{Edge Nodes}, \textit{Standard Nodes} and \textit{Cloud}. \textit{Edge Node} are small computational units distributed at the edge (e.g. Raspberry Pis), \textit{Standard Nodes} may be any arbitrary bigger server-site. The \textit{Cloud} is a big datacenter that -- in this model -- is expected to have virtually infinite resources. Paths in different colors take different routes (to the Cloud) and emphasize the unpredictability of resources when making a request in a fog system.}
    \label{fig:fog-infrastructure}
\end{figure}

Deployments in large datacenters and at the edge are conceptually considerably different and offer their respective set of challenges. 
Nevertheless, one can make assumptions about resource availability in the corresponding environments. 
To illustrate, a deployment in the Cloud is expected to have virtually infinite resources, which mostly function reliably \cite{288770}, but a deployment at the edge is expected to have very little, often unreliable resources \cite{9657141}. 
However, fog computing resources are often heterogeneous, as shown in Figure~\ref{fig:fog-infrastructure}, with servers closer to the Edge typically possessing lower computational capacity.
Consequently, we cannot make the same assumptions about resources here, as we could above. \cite{10.1145/2677046.2677052}

However, Fog Computing offers three advantages that can be derived from its common setup. 
Firstly, it promises lower latency by not exclusively running customer code in few, potentially geographically distant datacenters or at the edge where resources are scarce but additionally on infrastructure at the edge and in between. \cite{10.1145/2677046.2677052}
Secondly, it allows for enhanced privacy since user data can be processed relatively local and decentralized. \cite{10.1145/2677046.2677052}
Lastly, by reducing the mean geographical distance between client and server, it minimizes global network bandwidth consumption. \cite{https://doi.org/10.1002/spe.3058}

Furthermore, FaaS is traditionally run at a single layer meaning that although requests may be routed through multiple load balancers, proxies, etc., they are only processed at one level of the call graph. 
Fog computing, on the other hand, commonly uses resources as proxies and servers simultaneously\footnote{Server is a broad term, but here it intends to describe a computing instance that serves requests}. 
This is often implemented by checking resource availability at request arrival and forwarding it if resources are insufficient for execution. \cite{https://doi.org/10.1002/spe.3058}

% Rephrase this better -> maybe this also belongs in my own part?
% For the aforementioned established deployments this means, that assumptions on resources can be made and one of two cases is expected: 
% Either enough resources are available for a request to be executed instantly or requests are queued. From an architecture perspective this is reasonable since there is only one system layer capable of request execution, and we expect it to provide sufficient resources to handle the load.
% Contrarily, for FaaS in a fog environment, a request is forwarded instead of queued, since there are multiple serving layers, and it can not be assumed that a particular one is able to handle all load. 

